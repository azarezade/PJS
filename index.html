<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Sparse Visual Tracking by azarezade</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Sparse Visual Tracking</h1>
      <h2 class="project-tagline">MATLAB Implementations and Dataset</h2>
      <a href="https://github.com/azarezade/Patchwise-Joint-Sparse-Tracking" class="btn">View on GitHub</a>
      <a href="https://github.com/azarezade/Patchwise-Joint-Sparse-Tracking/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/azarezade/Patchwise-Joint-Sparse-Tracking/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="visual-tracking-via-joint-sparse-representation" class="anchor" href="#visual-tracking-via-joint-sparse-representation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visual Tracking via Joint Sparse Representation</h3>

<p>Given a bounding box defining the object of interest (target) in the first frame of a video sequence, the goal of a tracker, is to determine the object’s bounding box in subsequent frames. In contrast to specific  trackers, where the object model is learned off-line,  general tracking is more challenging since the object is previously unknown and needs to be learned throughout the video sequence.  Primary challenges encountered in visual tracking are target appearance change and occlusion, while other challenges arise from variation in illumination, scale, and camera motion.</p>

<p>Sparse representation has recently shown appealing results in various computer vision applications.  Generally, a candidate is represented using a linear combination of a few elements (atoms) from a dictionary composed of a number of previously found target images. The coefficients of this representation are used to find the best candidate. Apart from the ability to handle illumination and mild pose change, these trackers attempt to tackle occlusion.</p>

<p><img src="http://ssp.dml.ir/wp-content/uploads/2013/11/grouping-1024x229.png" alt="joint sparse illustration"></p>

<p>This paper presents a robust tracking approach to handle challenges such as occlusion and appearance change. The target is partitioned into a number of patches. The appearance of each patch is modeled using a dictionary composed of corresponding target patches in previous frames. In each frame, the target is found among a set of candidates generated by a particle filter, via a likelihood measure that is shown to be proportional to the sum of patch-reconstruction errors of each candidate. Since the target’s appearance often changes slowly in a video sequence, it is assumed that the target in the current frame and the best candidates of a small number of previous frames, belong to a common subspace. This is imposed using joint sparse representation to enforce the target and previous best candidates to have a common sparsity pattern. Moreover, an occlusion detection scheme is proposed that uses patch-reconstruction errors and a prior probability of occlusion, extracted from an adaptive Markov chain, to calculate the probability of occlusion per patch. In each frame, occluded patches are excluded when updating the dictionary. Extensive experimental results on several challenging sequences shows that the proposed method outperforms state-of-the-art trackers.</p>

<h3>
<a id="related-publication" class="anchor" href="#related-publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related Publication</h3>

<ul>
<li>Zarezade A., Rabiee H. R., Soltani-Farani A., and Khajenezhad A., “Patchwise Joint Sparse Tracking with Occlusion Detection”, <em>IEEE Transactions on Image Processing (TIP)</em>, 2014. <a href="http://ieeexplore.ieee.org/document/6873285/">link</a>
</li>
<li>Soltani-Farani, Ali, Hamid R. Rabiee, and Ali Zarezade. "Collaborating frames: Temporally weighted sparse representation for visual tracking.", <em>IEEE International Conference on Image Processing (ICIP)</em>, 2014. <a href="http://ieeexplore.ieee.org/document/7025091/">link</a>
</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/azarezade/Patchwise-Joint-Sparse-Tracking">Sparse Visual Tracking</a> is maintained by <a href="https://github.com/azarezade">azarezade</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
